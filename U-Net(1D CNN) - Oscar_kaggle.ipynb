{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this kernel, I'd like to share the approach using 1D Convolutional Neural Networks(1D CNN). 1D CNN is sometimes effective to analyze time series data. This kernel introduces U-Net architecture with 1D CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import warnings\n",
    "import time\n",
    "import pywt\n",
    "import random as rn\n",
    "from numpy.fft import *\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from tensorflow.keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import constraints\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.layers import multiply\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.layers import Input,Dropout,BatchNormalization,Activation,add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Dataset\n",
    "Simply split the input data into certain length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_signal(signal, threshold=1e7):\n",
    "#     fourier = rfft(signal)\n",
    "# #     print(fourier.shape)\n",
    "#     frequencies = rfftfreq(signal.size, d=20e-6/signal.size)\n",
    "# #     print(frequencies.shape)\n",
    "#     fourier[frequencies > threshold] = 0\n",
    "#     return irfft(fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal = np.array([[2, 3, 1], [4, 5, 6], [8, 2, 3], [2, 5, 6]])\n",
    "# print(filter_signal(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\n",
    "\n",
    "# print(df_train.shape)\n",
    "\n",
    "# I don't use \"time\" feature\n",
    "# train_input0 = filter_signal(df_train[\"signal\"].values)\n",
    "# train_input = train_input0.reshape(-1, 4000, 1)\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.plot(df_train[\"signal\"], label='Raw')\n",
    "# plt.plot(train_input0, label='Filtered')\n",
    "# plt.show()\n",
    "train_input = df_train[\"signal\"].values.reshape(-1,4000,1)#number_of_data:1250 x time_step:4000\n",
    "print(train_input.shape)\n",
    "train_input_mean = train_input.mean()\n",
    "train_input_sigma = train_input.std()\n",
    "train_input = (train_input-train_input_mean)/train_input_sigma\n",
    "print(df_test[\"signal\"].values.shape)\n",
    "test_input = df_test[\"signal\"].values.reshape(-1,4000,1)\n",
    "test_input = (test_input-train_input_mean)/train_input_sigma\n",
    "\n",
    "train_target = pd.get_dummies(df_train[\"open_channels\"]).values.reshape(-1,4000,11)\n",
    "idx = np.arange(train_input.shape[0])\n",
    "train_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\n",
    "# print(train_idx[:20])\n",
    "val_input = train_input[val_idx]\n",
    "# val_input = filter_signal(val_input)\n",
    "train_input = train_input[train_idx] \n",
    "# train_input = filter_signal(train_input)\n",
    "val_target = train_target[val_idx]\n",
    "train_target = train_target[train_idx] \n",
    "\n",
    "print(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape[0], val_input.shape, train_target.shape, val_target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "This section defines U-Net(se-resnet base).\n",
    "Input and output of the U-Net are follows:\n",
    "* Input: 4000 time steps of \"signal\"\n",
    "* Output: 4000 time steps of \"open_channels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convolution_block(x, filters, size, strides=1, padding='same', activation=True):\n",
    "#     x = Conv1D(filters, size, strides=strides, padding=padding)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     if activation == True:\n",
    "#         x = LeakyReLU(alpha=0.1)(x)\n",
    "#     return x\n",
    "\n",
    "# def residual_block(blockInput, num_filters=16):\n",
    "#     x = LeakyReLU(alpha=0.1)(blockInput)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     blockInput = BatchNormalization()(blockInput)\n",
    "#     x = convolution_block(x, num_filters, 7 )\n",
    "#     x = convolution_block(x, num_filters, 7, activation=False)\n",
    "#     x = Add()([x, blockInput])\n",
    "#     return x\n",
    "\n",
    "# def upsample(x):\n",
    "#     u = UpSampling1D(10)(x)\n",
    "#     return u\n",
    "\n",
    "# def downsample(x):\n",
    "#     x = AveragePooling1D(10)(x)\n",
    "#     return x\n",
    "\n",
    "# # a is conv in same layer\n",
    "# # d is downsample\n",
    "# # u is upsample\n",
    "# def DenseHRNet(input_shape=(None, 1)):\n",
    "#     layer_n = 64\n",
    "#     kernel_size = 7\n",
    "#     depth = 2\n",
    "    \n",
    "#     input_layer = Input(input_shape)\n",
    "#     x = input_layer\n",
    "#     for i in range(depth):\n",
    "#         x = residual_block(x)\n",
    "#     l1_1 = x\n",
    "#     l1_2 = residual_block(l1_1)\n",
    "#     l1_3 = residual_block(l1_2)\n",
    "    \n",
    "#     l2_1 = downsample(l1_1)\n",
    "#     l2_2 = residual_block(l2_1)\n",
    "    \n",
    "#     a1_4 = residual_block(l1_3)\n",
    "#     u2_2 = upsample(l2_2)\n",
    "#     l1_4 = Concatenate()([a1_4, u2_2])\n",
    "    \n",
    "#     l1_5 = residual_block(l1_4, num_filters=32)\n",
    "    \n",
    "#     d1_3 = downsample(l1_4)\n",
    "#     a2_2 = residual_block(l2_2)\n",
    "#     l2_3 = Concatenate()([d1_3, a2_2]) \n",
    "    \n",
    "#     l2_4 = residual_block(l2_3, num_filters=48)\n",
    "    \n",
    "#     d1_5 = downsample(l1_5)\n",
    "#     a2_4 = residual_block(l2_4, num_filters=48)\n",
    "#     l2_5 = Concatenate()([d1_5, a2_4])\n",
    "    \n",
    "#     a1_5 = residual_block(l1_5, num_filters=32)\n",
    "#     u2_4 = upsample(l2_4)\n",
    "#     u2_5 = upsample(l2_5)\n",
    "#     l1_6 = Concatenate()([a1_5, u2_4, u2_5])\n",
    "    \n",
    "#     x = Conv1D(11, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
    "#     out = Activation(\"softmax\")(x)\n",
    "#     model = Model(input_layer, out)\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def augmentations(input_data, target_data):\n",
    "#     #flip\n",
    "#     if np.random.rand()<0.5:    \n",
    "#         input_data = input_data[::-1]\n",
    "#         target_data = target_data[::-1]\n",
    "\n",
    "#     return input_data, target_data\n",
    "\n",
    "\n",
    "# def Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n",
    "#     x=[]\n",
    "#     y=[]\n",
    "  \n",
    "#     count=0\n",
    "#     idx_1 = np.arange(len(input_dataset))\n",
    "#     #idx_2 = np.arange(len(input_dataset))\n",
    "#     np.random.shuffle(idx_1)\n",
    "#     #np.random.shuffle(idx_2)\n",
    "\n",
    "#     while True:\n",
    "#         for i in range(len(input_dataset)):\n",
    "#             input_data = input_dataset[idx_1[i]]\n",
    "#             target_data = target_dataset[idx_1[i]]\n",
    "#             #input_data_mix = input_dataset[idx_2[i]]\n",
    "#             #target_data_mix = target_dataset[idx_2[i]]\n",
    "\n",
    "#             if is_train:\n",
    "#                 input_data, target_data = augmentations(input_data, target_data)\n",
    "#                 #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n",
    "                \n",
    "#             x.append(input_data)\n",
    "#             y.append(target_data)\n",
    "#             count+=1\n",
    "#             if count==batch_size:\n",
    "#                 x=np.array(x, dtype=np.float32)\n",
    "#                 y=np.array(y, dtype=np.float32)\n",
    "#                 inputs = x\n",
    "#                 targets = y       \n",
    "#                 x = []\n",
    "#                 y = []\n",
    "#                 count=0\n",
    "#                 yield inputs, targets\n",
    "\n",
    "# class macroF1(Callback):\n",
    "#     def __init__(self, model, inputs, targets):\n",
    "#         self.model = model\n",
    "#         self.inputs = inputs\n",
    "#         self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "#         f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "#         print(\"val_f1_macro_score: \", f1_val)\n",
    "        \n",
    "# def model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n",
    "#     hist = model.fit_generator(\n",
    "#         Datagen(train_inputs, train_targets, batch_size, is_train=True),\n",
    "#         steps_per_epoch = len(train_inputs) // batch_size,\n",
    "#         epochs = n_epoch,\n",
    "#         validation_data=Datagen(val_inputs, val_targets, batch_size),\n",
    "#         validation_steps = len(val_inputs) // batch_size,\n",
    "#         callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n",
    "#         shuffle = False,\n",
    "#         verbose = 1\n",
    "#         )\n",
    "#     return hist\n",
    "\n",
    "\n",
    "# def lrs(epoch):\n",
    "#     if epoch<35:\n",
    "#         lr = learning_rate\n",
    "#     elif epoch<50:\n",
    "#         lr = learning_rate/10\n",
    "#     elif epoch<70:\n",
    "#         lr = learning_rate/100\n",
    "#     else:\n",
    "#         lr = learning_rate/1000\n",
    "#     return lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# def cbr(x, out_layer, kernel, stride, dilation):\n",
    "#     x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     return x\n",
    "\n",
    "# def se_block(x_in, layer_n):\n",
    "#     x = GlobalAveragePooling1D()(x_in)\n",
    "#     x = Dense(layer_n//8, activation=\"relu\")(x)\n",
    "#     x = Dense(layer_n, activation=\"sigmoid\")(x)\n",
    "#     x_out=Multiply()([x_in, x])\n",
    "#     return x_out\n",
    "\n",
    "# def resblock(x_in, layer_n, kernel, dilation, use_se=True):\n",
    "#     x = cbr(x_in, layer_n, kernel, 1, dilation)\n",
    "#     x = cbr(x, layer_n, kernel, 1, dilation)\n",
    "#     if use_se:\n",
    "#         x = se_block(x, layer_n)\n",
    "#     x = Add()([x_in, x])\n",
    "#     return x  \n",
    "\n",
    "# def UUnet(input_shape=(None,1)):\n",
    "#     layer_n = 64\n",
    "#     kernel_size = 7\n",
    "#     depth = 2\n",
    "\n",
    "#     input_layer = Input(input_shape)    \n",
    "#     input_layer_1 = AveragePooling1D(5)(input_layer)\n",
    "#     input_layer_2 = AveragePooling1D(25)(input_layer)\n",
    "    \n",
    "#     ##########################\n",
    "#     ### First U-Net: Big-U ###\n",
    "#     ##########################\n",
    "    \n",
    "#     ########## Encoder 1\n",
    "#     x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n, kernel_size, 1)\n",
    "#     out_0 = x\n",
    "\n",
    "#     x = cbr(x, layer_n*2, kernel_size, 5, 1)\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n*2, kernel_size, 1)\n",
    "#     out_1 = x\n",
    "\n",
    "#     x = Concatenate()([x, input_layer_1])    \n",
    "#     x = cbr(x, layer_n*3, kernel_size, 5, 1)\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n*3, kernel_size, 1)\n",
    "#     out_2 = x\n",
    "\n",
    "#     x = Concatenate()([x, input_layer_2])    \n",
    "#     x = cbr(x, layer_n*4, kernel_size, 5, 1)\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n*4, kernel_size, 1)\n",
    "    \n",
    "#     ########### Decoder 1\n",
    "#     x = UpSampling1D(5)(x)\n",
    "#     x = Concatenate()([x, out_2])\n",
    "#     x = cbr(x, layer_n*3, kernel_size, 1, 1)\n",
    "\n",
    "#     x = UpSampling1D(5)(x)\n",
    "#     x = Concatenate()([x, out_1])\n",
    "#     x = cbr(x, layer_n*2, kernel_size, 1, 1)\n",
    "\n",
    "#     x = UpSampling1D(5)(x)\n",
    "#     x = Concatenate()([x, out_0])\n",
    "#     x = cbr(x, layer_n, kernel_size, 1, 1)   \n",
    "    \n",
    "#     #############################\n",
    "#     ### Second U-Net: Small-U ###\n",
    "#     #############################\n",
    "    \n",
    "#     ########## Encoder 2\n",
    "#     #x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n, kernel_size, 1)\n",
    "#     out_0 = x\n",
    "\n",
    "#     x = cbr(x, layer_n*2, kernel_size, 5, 1)\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n*2, kernel_size, 1)\n",
    "#     out_1 = x\n",
    "\n",
    "#     x = Concatenate()([x, input_layer_1])    \n",
    "#     x = cbr(x, layer_n*3, kernel_size, 5, 1)\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n*3, kernel_size, 1)\n",
    "    \n",
    "#     ########### Decoder 2\n",
    "#     x = UpSampling1D(5)(x)\n",
    "#     x = Concatenate()([x, out_1])\n",
    "#     x = cbr(x, layer_n*2, kernel_size, 1, 1)\n",
    "\n",
    "#     x = UpSampling1D(5)(x)\n",
    "#     x = Concatenate()([x, out_0])\n",
    "#     x = cbr(x, layer_n, kernel_size, 1, 1) \n",
    "    \n",
    "#     ###############################\n",
    "#     ### Third U-Net: Smallest-U ###\n",
    "#     ###############################\n",
    "    \n",
    "#     ########## Encoder 2\n",
    "#     #x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n, kernel_size, 1)\n",
    "#     out_0 = x\n",
    "    \n",
    "#     x = cbr(x, layer_n*2, kernel_size, 5, 1)\n",
    "#     for i in range(depth):\n",
    "#         x = resblock(x, layer_n*2, kernel_size, 1)\n",
    "    \n",
    "#     ########### Decoder 2\n",
    "#     x = UpSampling1D(5)(x)\n",
    "#     x = Concatenate()([x, out_0])\n",
    "#     x = cbr(x, layer_n, kernel_size, 1, 1)\n",
    "    \n",
    "#     #classifier\n",
    "#     x = Conv1D(11, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
    "#     out = Activation(\"softmax\")(x)\n",
    "    \n",
    "#     model = Model(input_layer, out)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "# def augmentations(input_data, target_data):\n",
    "#     #flip\n",
    "#     if np.random.rand()<0.5:    \n",
    "#         input_data = input_data[::-1]\n",
    "#         target_data = target_data[::-1]\n",
    "\n",
    "#     return input_data, target_data\n",
    "\n",
    "\n",
    "# def Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n",
    "#     x=[]\n",
    "#     y=[]\n",
    "  \n",
    "#     count=0\n",
    "#     idx_1 = np.arange(len(input_dataset))\n",
    "#     #idx_2 = np.arange(len(input_dataset))\n",
    "#     np.random.shuffle(idx_1)\n",
    "#     #np.random.shuffle(idx_2)\n",
    "\n",
    "#     while True:\n",
    "#         for i in range(len(input_dataset)):\n",
    "#             input_data = input_dataset[idx_1[i]]\n",
    "#             target_data = target_dataset[idx_1[i]]\n",
    "#             #input_data_mix = input_dataset[idx_2[i]]\n",
    "#             #target_data_mix = target_dataset[idx_2[i]]\n",
    "\n",
    "#             if is_train:\n",
    "#                 input_data, target_data = augmentations(input_data, target_data)\n",
    "#                 #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n",
    "                \n",
    "#             x.append(input_data)\n",
    "#             y.append(target_data)\n",
    "#             count+=1\n",
    "#             if count==batch_size:\n",
    "#                 x=np.array(x, dtype=np.float32)\n",
    "#                 y=np.array(y, dtype=np.float32)\n",
    "#                 inputs = x\n",
    "#                 targets = y       \n",
    "#                 x = []\n",
    "#                 y = []\n",
    "#                 count=0\n",
    "#                 yield inputs, targets\n",
    "\n",
    "# class macroF1(Callback):\n",
    "#     def __init__(self, model, inputs, targets):\n",
    "#         self.model = model\n",
    "#         self.inputs = inputs\n",
    "#         self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "#         f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "#         print(\"val_f1_macro_score: \", f1_val)\n",
    "        \n",
    "# def model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n",
    "#     hist = model.fit_generator(\n",
    "#         Datagen(train_inputs, train_targets, batch_size, is_train=True),\n",
    "#         steps_per_epoch = len(train_inputs) // batch_size,\n",
    "#         epochs = n_epoch,\n",
    "#         validation_data=Datagen(val_inputs, val_targets, batch_size),\n",
    "#         validation_steps = len(val_inputs) // batch_size,\n",
    "#         callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n",
    "#         shuffle = False,\n",
    "#         verbose = 1\n",
    "#         )\n",
    "#     return hist\n",
    "\n",
    "\n",
    "# def lrs(epoch):\n",
    "#     if epoch<30:\n",
    "#         lr = learning_rate\n",
    "#     elif epoch<50:\n",
    "#         lr = learning_rate/10\n",
    "#     elif epoch<70:\n",
    "#         lr = learning_rate/100\n",
    "#     elif epoch<110:\n",
    "#         lr = learning_rate/1000\n",
    "#     elif epoch<170:\n",
    "#         lr = learning_rate/10000\n",
    "#     else:\n",
    "#         lr = learning_rate/50000\n",
    "#     return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbr(x, out_layer, kernel, stride, dilation):\n",
    "    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def se_block(x_in, layer_n):\n",
    "    x = GlobalAveragePooling1D()(x_in)\n",
    "    x = Dense(layer_n//8, activation=\"relu\")(x)\n",
    "    x = Dense(layer_n, activation=\"sigmoid\")(x)\n",
    "    x_out=Multiply()([x_in, x])\n",
    "    return x_out\n",
    "\n",
    "def resblock(x_in, layer_n, kernel, dilation, use_se=True):\n",
    "    x = cbr(x_in, layer_n, kernel, 1, dilation)\n",
    "    x = cbr(x, layer_n, kernel, 1, dilation)\n",
    "    if use_se:\n",
    "        x = se_block(x, layer_n)\n",
    "    x = Add()([x_in, x])\n",
    "    return x  \n",
    "\n",
    "def UUnet(input_shape=(None,1)):\n",
    "    layer_n = 64\n",
    "    kernel_size1 = 7\n",
    "    kernel_size2 = 5\n",
    "    kernel_size3 = 3\n",
    "    depth = 3\n",
    "\n",
    "    input_layer = Input(input_shape)    \n",
    "    input_layer_1 = AveragePooling1D(5)(input_layer)\n",
    "    input_layer_2 = AveragePooling1D(25)(input_layer)\n",
    "    \n",
    "    ##########################\n",
    "    ### First U-Net: Big-U ###\n",
    "    ##########################\n",
    "    \n",
    "    ########## Encoder 1\n",
    "    x = cbr(input_layer, layer_n, kernel_size1, 1, 1)#1000\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n, kernel_size1, 1)\n",
    "    out_0 = x\n",
    "\n",
    "    x = cbr(x, layer_n*2, kernel_size2, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*2, kernel_size2, 1)\n",
    "    out_1 = x\n",
    "\n",
    "    x = Concatenate()([x, input_layer_1])    \n",
    "    x = cbr(x, layer_n*3, kernel_size3, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*3, kernel_size3, 1)\n",
    "    out_2 = x\n",
    "\n",
    "    x = Concatenate()([x, input_layer_2])    \n",
    "    x = cbr(x, layer_n*4, kernel_size3, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*4, kernel_size3, 1)\n",
    "    \n",
    "    ########### Decoder 1\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_2])\n",
    "    x = cbr(x, layer_n*3, kernel_size3, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_1])\n",
    "    x = cbr(x, layer_n*2, kernel_size2, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_0])\n",
    "    x = cbr(x, layer_n, kernel_size1, 1, 1)   \n",
    "    \n",
    "    #############################\n",
    "    ### Second U-Net: Small-U ###\n",
    "    #############################\n",
    "    \n",
    "    ########## Encoder 2\n",
    "    #x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n, kernel_size1, 1)\n",
    "    out_0 = x\n",
    "\n",
    "    x = cbr(x, layer_n*2, kernel_size2, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*2, kernel_size2, 1)\n",
    "    out_1 = x\n",
    "\n",
    "    x = Concatenate()([x, input_layer_1])    \n",
    "    x = cbr(x, layer_n*3, kernel_size3, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*3, kernel_size3, 1)\n",
    "    \n",
    "    ########### Decoder 2\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_1])\n",
    "    x = cbr(x, layer_n*2, kernel_size3, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_0])\n",
    "    x = cbr(x, layer_n, kernel_size2, 1, 1) \n",
    "    \n",
    "    ###############################\n",
    "    ### Third U-Net: Small-U ###\n",
    "    ###############################\n",
    "    \n",
    "    ########## Encoder 3\n",
    "    #x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n, kernel_size1, 1)\n",
    "    out_0 = x\n",
    "\n",
    "    x = cbr(x, layer_n*2, kernel_size2, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*2, kernel_size2, 1)\n",
    "    out_1 = x\n",
    "\n",
    "    x = Concatenate()([x, input_layer_1])    \n",
    "    x = cbr(x, layer_n*3, kernel_size3, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*3, kernel_size3, 1)\n",
    "    \n",
    "    ########### Decoder 2\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_1])\n",
    "    x = cbr(x, layer_n*2, kernel_size3, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_0])\n",
    "    x = cbr(x, layer_n, kernel_size2, 1, 1) \n",
    "    \n",
    "    ###############################\n",
    "    ### Forth U-Net: big-U ###\n",
    "    ###############################\n",
    "    \n",
    "    ########## Encoder 4\n",
    "    x = cbr(input_layer, layer_n, kernel_size1, 1, 1)#1000\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n, kernel_size1, 1)\n",
    "    out_0 = x\n",
    "\n",
    "    x = cbr(x, layer_n*2, kernel_size2, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*2, kernel_size2, 1)\n",
    "    out_1 = x\n",
    "\n",
    "    x = Concatenate()([x, input_layer_1])    \n",
    "    x = cbr(x, layer_n*3, kernel_size3, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*3, kernel_size3, 1)\n",
    "    out_2 = x\n",
    "\n",
    "    x = Concatenate()([x, input_layer_2])    \n",
    "    x = cbr(x, layer_n*4, kernel_size3, 5, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*4, kernel_size3, 1)\n",
    "    \n",
    "    ########### Decoder 4\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_2])\n",
    "    x = cbr(x, layer_n*3, kernel_size3, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_1])\n",
    "    x = cbr(x, layer_n*2, kernel_size2, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Concatenate()([x, out_0])\n",
    "    x = cbr(x, layer_n, kernel_size1, 1, 1)  \n",
    "    \n",
    "    #classifier\n",
    "    x = Conv1D(11, kernel_size=kernel_size2, strides=1, padding=\"same\")(x)\n",
    "    out = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(input_layer, out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def augmentations(input_data, target_data):\n",
    "    #flip\n",
    "    if np.random.rand()<0.5:    \n",
    "        input_data = input_data[::-1]\n",
    "        target_data = target_data[::-1]\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "\n",
    "def Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n",
    "    x=[]\n",
    "    y=[]\n",
    "  \n",
    "    count=0\n",
    "    idx_1 = np.arange(len(input_dataset))\n",
    "    #idx_2 = np.arange(len(input_dataset))\n",
    "    np.random.shuffle(idx_1)\n",
    "    #np.random.shuffle(idx_2)\n",
    "\n",
    "    while True:\n",
    "        for i in range(len(input_dataset)):\n",
    "            input_data = input_dataset[idx_1[i]]\n",
    "            target_data = target_dataset[idx_1[i]]\n",
    "            #input_data_mix = input_dataset[idx_2[i]]\n",
    "            #target_data_mix = target_dataset[idx_2[i]]\n",
    "\n",
    "            if is_train:\n",
    "                input_data, target_data = augmentations(input_data, target_data)\n",
    "                #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n",
    "                \n",
    "            x.append(input_data)\n",
    "            y.append(target_data)\n",
    "            count+=1\n",
    "            if count==batch_size:\n",
    "                x=np.array(x, dtype=np.float32)\n",
    "                y=np.array(y, dtype=np.float32)\n",
    "                inputs = x\n",
    "                targets = y       \n",
    "                x = []\n",
    "                y = []\n",
    "                count=0\n",
    "                yield inputs, targets\n",
    "\n",
    "class macroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "        f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "        print(\"val_f1_macro_score: \", f1_val)\n",
    "        \n",
    "def model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n",
    "    hist = model.fit_generator(\n",
    "        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n",
    "        steps_per_epoch = len(train_inputs) // batch_size,\n",
    "        epochs = n_epoch,\n",
    "        validation_data=Datagen(val_inputs, val_targets, batch_size),\n",
    "        validation_steps = len(val_inputs) // batch_size,\n",
    "        callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n",
    "        shuffle = False,\n",
    "        verbose = 1\n",
    "        )\n",
    "    return hist\n",
    "\n",
    "\n",
    "def lrs(epoch):\n",
    "    if epoch<45:\n",
    "        lr = learning_rate\n",
    "    elif epoch<75:\n",
    "        lr = learning_rate/10\n",
    "    elif epoch<115:\n",
    "        lr = learning_rate/100\n",
    "    elif epoch<165:\n",
    "        lr = learning_rate/1000\n",
    "#     elif epoch<170:\n",
    "#         lr = learning_rate/10000\n",
    "    else:\n",
    "        lr = learning_rate/10000\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv3x3(x, out_filters, strides=1):\n",
    "#     x = Conv1D(out_filters, 3, padding='same', strides=strides, use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def basic_Block(input, out_filters, strides=1, with_conv_shortcut=False):\n",
    "#     x = conv3x3(input, out_filters, strides)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "#     x = Activation('relu')(x)\n",
    "\n",
    "#     x = conv3x3(x, out_filters)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "#     if with_conv_shortcut:\n",
    "#         residual = Conv1D(out_filters, 1, strides=strides, use_bias=False, kernel_initializer='he_normal')(input)\n",
    "#         residual = BatchNormalization(axis=-1)(residual)\n",
    "#         x = add([x, residual])\n",
    "#     else:\n",
    "#         x = add([x, input])\n",
    "\n",
    "#     x = Activation('relu')(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def bottleneck_Block(input, out_filters, strides=1, with_conv_shortcut=False):\n",
    "#     expansion = 4\n",
    "#     de_filters = int(out_filters / expansion)\n",
    "\n",
    "#     x = Conv1D(de_filters, 1, use_bias=False, kernel_initializer='he_normal')(input)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "#     x = Activation('relu')(x)\n",
    "\n",
    "#     x = Conv1D(de_filters, 3, strides=strides, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "#     x = Activation('relu')(x)\n",
    "\n",
    "#     x = Conv1D(out_filters, 1, use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "#     if with_conv_shortcut:\n",
    "#         residual = Conv1D(out_filters, 1, strides=strides, use_bias=False, kernel_initializer='he_normal')(input)\n",
    "#         residual = BatchNormalization(axis=-1)(residual)\n",
    "#         x = add([x, residual])\n",
    "#     else:\n",
    "#         x = add([x, input])\n",
    "\n",
    "#     x = Activation('relu')(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def stem_net(input):\n",
    "#     x = Conv1D(64, 3, strides=2, padding='same', use_bias=False, kernel_initializer='he_normal')(input)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "#     x = Activation('relu')(x)\n",
    "\n",
    "#     # x = Conv2D(64, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     # x = BatchNormalization(axis=3)(x)\n",
    "#     # x = Activation('relu')(x)\n",
    "\n",
    "#     x = bottleneck_Block(x, 256, with_conv_shortcut=True)\n",
    "#     x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n",
    "#     x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n",
    "#     x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n",
    "\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def transition_layer1(x, out_filters_list=[32, 64]):\n",
    "#     x0 = Conv1D(out_filters_list[0], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     x0 = BatchNormalization(axis=-1)(x0)\n",
    "#     x0 = Activation('relu')(x0)\n",
    "\n",
    "#     x1 = Conv1D(out_filters_list[1], 3, strides=2,\n",
    "#                 padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     x1 = BatchNormalization(axis=-1)(x1)\n",
    "#     x1 = Activation('relu')(x1)\n",
    "\n",
    "#     return [x0, x1]\n",
    "\n",
    "\n",
    "# def make_branch1_0(x, out_filters=32):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def make_branch1_1(x, out_filters=64):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def fuse_layer1(x):\n",
    "#     x0_0 = x[0]\n",
    "#     x0_1 = Conv1D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x0_1 = BatchNormalization(axis=-1)(x0_1)\n",
    "#     x0_1 = UpSampling1D(size=2)(x0_1)\n",
    "#     x0 = add([x0_0, x0_1])\n",
    "\n",
    "#     x1_0 = Conv1D(64, 3, strides=2, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n",
    "#     x1_0 = BatchNormalization(axis=-1)(x1_0)\n",
    "#     x1_1 = x[1]\n",
    "#     x1 = add([x1_0, x1_1])\n",
    "#     return [x0, x1]\n",
    "\n",
    "\n",
    "# def transition_layer2(x, out_filters_list=[32, 64, 128]):\n",
    "#     x0 = Conv1D(out_filters_list[0], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n",
    "#     x0 = BatchNormalization(axis=-1)(x0)\n",
    "#     x0 = Activation('relu')(x0)\n",
    "\n",
    "#     x1 = Conv1D(out_filters_list[1], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x1 = BatchNormalization(axis=-1)(x1)\n",
    "#     x1 = Activation('relu')(x1)\n",
    "\n",
    "#     x2 = Conv1D(out_filters_list[2], 3, strides=2,\n",
    "#                 padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x2 = BatchNormalization(axis=-1)(x2)\n",
    "#     x2 = Activation('relu')(x2)\n",
    "\n",
    "#     return [x0, x1, x2]\n",
    "\n",
    "\n",
    "# def make_branch2_0(x, out_filters=32):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def make_branch2_1(x, out_filters=64):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def make_branch2_2(x, out_filters=128):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def fuse_layer2(x):\n",
    "#     x0_0 = x[0]\n",
    "#     x0_1 = Conv1D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x0_1 = BatchNormalization(axis=-1)(x0_1)\n",
    "#     x0_1 = UpSampling1D(size=2)(x0_1)\n",
    "#     x0_2 = Conv1D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[2])\n",
    "#     x0_2 = BatchNormalization(axis=-1)(x0_2)\n",
    "#     x0_2 = UpSampling1D(size=4)(x0_2)\n",
    "#     x0 = add([x0_0, x0_1, x0_2])\n",
    "\n",
    "#     x1_0 = Conv1D(64, 3, strides=2, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n",
    "#     x1_0 = BatchNormalization(axis=-1)(x1_0)\n",
    "#     x1_1 = x[1]\n",
    "#     x1_2 = Conv1D(64, 1, use_bias=False, kernel_initializer='he_normal')(x[2])\n",
    "#     x1_2 = BatchNormalization(axis=-1)(x1_2)\n",
    "#     x1_2 = UpSampling1D(size=2)(x1_2)\n",
    "#     x1 = add([x1_0, x1_1, x1_2])\n",
    "\n",
    "#     x2_0 = Conv1D(32, 3, strides=2, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n",
    "#     x2_0 = BatchNormalization(axis=-1)(x2_0)\n",
    "#     x2_0 = Activation('relu')(x2_0)\n",
    "#     x2_0 = Conv1D(128, 3, strides=2, padding='same', use_bias=False, kernel_initializer='he_normal')(x2_0)\n",
    "#     x2_0 = BatchNormalization(axis=-1)(x2_0)\n",
    "#     x2_1 = Conv1D(128, 3, strides=2, padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x2_1 = BatchNormalization(axis=-1)(x2_1)\n",
    "#     x2_2 = x[2]\n",
    "#     x2 = add([x2_0, x2_1, x2_2])\n",
    "#     return [x0, x1, x2]\n",
    "\n",
    "\n",
    "# def transition_layer3(x, out_filters_list=[32, 64, 128, 256]):\n",
    "#     x0 = Conv1D(out_filters_list[0], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n",
    "#     x0 = BatchNormalization(axis=-1)(x0)\n",
    "#     x0 = Activation('relu')(x0)\n",
    "\n",
    "#     x1 = Conv1D(out_filters_list[1], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x1 = BatchNormalization(axis=-1)(x1)\n",
    "#     x1 = Activation('relu')(x1)\n",
    "\n",
    "#     x2 = Conv1D(out_filters_list[2], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[2])\n",
    "#     x2 = BatchNormalization(axis=-1)(x2)\n",
    "#     x2 = Activation('relu')(x2)\n",
    "\n",
    "#     x3 = Conv1D(out_filters_list[3], 3, strides=2,\n",
    "#                 padding='same', use_bias=False, kernel_initializer='he_normal')(x[2])\n",
    "#     x3 = BatchNormalization(axis=-1)(x3)\n",
    "#     x3 = Activation('relu')(x3)\n",
    "\n",
    "#     return [x0, x1, x2, x3]\n",
    "\n",
    "\n",
    "# def make_branch3_0(x, out_filters=32):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def make_branch3_1(x, out_filters=64):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def make_branch3_2(x, out_filters=128):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def make_branch3_3(x, out_filters=256):\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def fuse_layer3(x):\n",
    "#     x0_0 = x[0]\n",
    "#     x0_1 = Conv1D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[1])\n",
    "#     x0_1 = BatchNormalization(axis=-1)(x0_1)\n",
    "#     x0_1 = UpSampling1D(size=2)(x0_1)\n",
    "#     x0_2 = Conv1D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[2])\n",
    "#     x0_2 = BatchNormalization(axis=-1)(x0_2)\n",
    "#     x0_2 = UpSampling1D(size=4)(x0_2)\n",
    "#     x0_3 = Conv1D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[3])\n",
    "#     x0_3 = BatchNormalization(axis=-1)(x0_3)\n",
    "#     x0_3 = UpSampling1D(size=8)(x0_3)\n",
    "#     x0 = concatenate([x0_0, x0_1, x0_2, x0_3], axis=-1)\n",
    "#     return x0\n",
    "\n",
    "\n",
    "# def final_layer(x, classes=11):\n",
    "#     x = UpSampling1D(size=2)(x)\n",
    "#     x = Conv1D(classes, 1, use_bias=False, kernel_initializer='he_normal')(x)\n",
    "#     x = BatchNormalization(axis=-1)(x)\n",
    "#     x = Activation('softmax', name='Classification')(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def seg_hrnet(input_shape=(None,1)):\n",
    "\n",
    "#     inputs = Input(input_shape)\n",
    "\n",
    "#     x = stem_net(inputs)\n",
    "\n",
    "#     x = transition_layer1(x)\n",
    "#     x0 = make_branch1_0(x[0])\n",
    "#     x1 = make_branch1_1(x[1])\n",
    "#     x = fuse_layer1([x0, x1])\n",
    "\n",
    "#     x = transition_layer2(x)\n",
    "#     x0 = make_branch2_0(x[0])\n",
    "#     x1 = make_branch2_1(x[1])\n",
    "#     x2 = make_branch2_2(x[2])\n",
    "#     x = fuse_layer2([x0, x1, x2])\n",
    "\n",
    "#     x = transition_layer3(x)\n",
    "#     x0 = make_branch3_0(x[0])\n",
    "#     x1 = make_branch3_1(x[1])\n",
    "#     x2 = make_branch3_2(x[2])\n",
    "#     x3 = make_branch3_3(x[3])\n",
    "#     x = fuse_layer3([x0, x1, x2, x3])\n",
    "\n",
    "#     out = final_layer(x, classes=11)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def augmentations(input_data, target_data):\n",
    "#     #flip\n",
    "#     if np.random.rand()<0.5:    \n",
    "#         input_data = input_data[::-1]\n",
    "#         target_data = target_data[::-1]\n",
    "\n",
    "#     return input_data, target_data\n",
    "\n",
    "\n",
    "# def Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n",
    "#     x=[]\n",
    "#     y=[]\n",
    "  \n",
    "#     count=0\n",
    "#     idx_1 = np.arange(len(input_dataset))\n",
    "#     #idx_2 = np.arange(len(input_dataset))\n",
    "#     np.random.shuffle(idx_1)\n",
    "#     #np.random.shuffle(idx_2)\n",
    "\n",
    "#     while True:\n",
    "#         for i in range(len(input_dataset)):\n",
    "#             input_data = input_dataset[idx_1[i]]\n",
    "#             target_data = target_dataset[idx_1[i]]\n",
    "#             #input_data_mix = input_dataset[idx_2[i]]\n",
    "#             #target_data_mix = target_dataset[idx_2[i]]\n",
    "\n",
    "#             if is_train:\n",
    "#                 input_data, target_data = augmentations(input_data, target_data)\n",
    "#                 #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n",
    "                \n",
    "#             x.append(input_data)\n",
    "#             y.append(target_data)\n",
    "#             count+=1\n",
    "#             if count==batch_size:\n",
    "#                 x=np.array(x, dtype=np.float32)\n",
    "#                 y=np.array(y, dtype=np.float32)\n",
    "#                 inputs = x\n",
    "#                 targets = y       \n",
    "#                 x = []\n",
    "#                 y = []\n",
    "#                 count=0\n",
    "#                 yield inputs, targets\n",
    "\n",
    "# class macroF1(Callback):\n",
    "#     def __init__(self, model, inputs, targets):\n",
    "#         self.model = model\n",
    "#         self.inputs = inputs\n",
    "#         self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "#         f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "#         print(\"val_f1_macro_score: \", f1_val)\n",
    "        \n",
    "# def model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n",
    "#     hist = model.fit_generator(\n",
    "#         Datagen(train_inputs, train_targets, batch_size, is_train=True),\n",
    "#         steps_per_epoch = len(train_inputs) // batch_size,\n",
    "#         epochs = n_epoch,\n",
    "#         validation_data=Datagen(val_inputs, val_targets, batch_size),\n",
    "#         validation_steps = len(val_inputs) // batch_size,\n",
    "#         callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n",
    "#         shuffle = False,\n",
    "#         verbose = 1\n",
    "#         )\n",
    "#     return hist\n",
    "\n",
    "\n",
    "# def lrs(epoch):\n",
    "#     if epoch<35:\n",
    "#         lr = learning_rate\n",
    "#     elif epoch<50:\n",
    "#         lr = learning_rate/100\n",
    "#     elif epoch<90:\n",
    "#         lr = learning_rate/1000\n",
    "#     elif epoch<150:\n",
    "#         lr = learning_rate/10000\n",
    "#     elif epoch<230:\n",
    "#         lr = learning_rate/100000\n",
    "#     else:\n",
    "#         lr = learning_rate/1000000\n",
    "#     return lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# model = DenseHRNet()\n",
    "model = UUnet()\n",
    "# print(model.summary())\n",
    "\n",
    "learning_rate=0.0005\n",
    "n_epoch=200\n",
    "# batch_size=100\n",
    "batch_size=8\n",
    "\n",
    "# earlystopping = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "lr_schedule = LearningRateScheduler(lrs)\n",
    "model.compile(loss=categorical_crossentropy, \n",
    "              optimizer=Adam(lr=learning_rate),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "hist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = ((model.predict(val_input)+model.predict(val_input[:,::-1,:])[:,::-1,:])/2).reshape(-1)\n",
    "# print(\"VALIDATION_SCORE: \", cohen_kappa_score(val_target.reshape(-1), np.round(pred, 0), weights=\"quadratic\"))\n",
    "pred = np.argmax((model.predict(val_input)+model.predict(val_input[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\n",
    "gt = np.argmax(val_target, axis=2).reshape(-1)\n",
    "print(\"SCORE_oldmetric: \", cohen_kappa_score(gt, pred, weights=\"quadratic\"))\n",
    "print(\"SCORE_newmetric: \", f1_score(gt, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Submit\n",
    "This is not the main topic of this kernel, so I just round predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax((model.predict(test_input)+model.predict(test_input[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\n",
    "\n",
    "df_sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n",
    "df_sub.open_channels = np.array(np.round(pred,0), np.int)\n",
    "df_sub.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
